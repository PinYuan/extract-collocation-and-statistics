{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# index對應到例子，若是片語來的例句，補上何片語\n",
    "file = open('index2exam.txt', 'w') \n",
    "\n",
    "# index對應到有哪些gp\n",
    "file1 = open('cam(index_GP).txt', 'w')\n",
    "\n",
    "unWantADV = {'by', 'maybe', 'evenly', 'as', 'eventually', 'elsewhere', 'soon', 'sometims', 'now', 'evetually', 'after', 'below', 'so', 'exactly', 'even', 'instead', 'merely', 'very', 'anymore', 'then', 'still', 'really', 'once', 'actually', 'ever', 'in', 'also', 'only', 'just', 'else', 'anytime', 'rather', 'indeed', 'again', 'of', 'likely', 'ago', 'further', 'all', 'likewise', 'already', 'thereby', 'together', 'enough', 'everyday', 'before', 'much', 'almost', 'altogether', 'anyway', 'always', 'everywhere', 'on', 'meanwhile', 'best', 'afterward'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAM need extract GP for every sentence map to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from extractGP import *\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def getRoot(dependency):\n",
    "    root = [ (iword, word, lemma, dep, tag, pos, ihead, NER) for iword, word, lemma, dep, tag, pos, ihead, NER in dependency if dep == 'ROOT' ]\n",
    "    if not root: return []\n",
    "    else: return root[0][0]\n",
    "\n",
    "def cleanCambridgeSent(sent):\n",
    "    sent = re.sub('\\[ [\\w +-]*\\]', '', sent)\n",
    "    for bracket in re.findall('(?P<word>\\([\\w ]*\\))', sent):\n",
    "        if bracket[1:3] in ['US', 'UK']: \n",
    "            sub_ = ' \\\\' + bracket[:-1] + '\\\\' + bracket[-1]\n",
    "            sent = re.sub(sub_, '', sent)\n",
    "        else:\n",
    "            sub_ = '\\\\' + bracket[:-1] + '\\\\' + bracket[-1]\n",
    "            sent = re.sub(sub_, bracket[1:-1], sent)\n",
    "    sent = re.sub(' \\(=.*\\)', '', sent)\n",
    "\n",
    "    for f, b in re.findall('(?P<f>\\w+)'+'/'+'(?P<b>\\w+)', sent):\n",
    "        sent = re.sub('/'+b, '', sent)\n",
    "    return sent.strip()\n",
    "\n",
    "def sentence_to_dependencyNER(sent):\n",
    "    dep = []\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    POSMapping = defaultdict(lambda: '')\n",
    "    for token in doc:\n",
    "        POSMapping[token.text] = token.pos_\n",
    "\n",
    "    NERMapping = defaultdict(lambda: ' ')\n",
    "    # by spacy\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in {'NORP', 'GPE', 'LANGUAGE', 'DATE', 'TIME', 'MONEY'}:\n",
    "            texts = [ word for word in ent.text.split() if POSMapping[word] in ['NUM', 'NOUN', 'PROPN'] ]\n",
    "            if texts:\n",
    "                if ent.label_ == 'DATE': NERMapping[texts[-1]] = 'TIME'\n",
    "                else: NERMapping[texts[-1]] = ent.label_\n",
    "    # by word itself\n",
    "    for token in doc:\n",
    "        upperWord = token.text.upper()\n",
    "        if upperWord in {'LANGUAGE', 'DATE', 'TIME', 'MONEY'} and NERMapping[token.text] == ' ':\n",
    "            NERMapping[token.text] = upperWord if upperWord != 'DATE' else 'TIME'\n",
    "\n",
    "    for token in doc:\n",
    "        dep += [(token.i, token.text, token.lemma_, token.dep_, token.tag_, token.pos_, token.head.i, NERMapping[token.text])]\n",
    "\n",
    "    return dep\n",
    "\n",
    "def extractCambGP(en):\n",
    "    dep = sentence_to_dependencyNER(cleanCambridgeSent(en))\n",
    "    GPs = dependency_to_GP(dep, getRoot(dep))\n",
    "    return GPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cambridgeDict = eval(open('camDict.txt', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "mapping = defaultdict(lambda: defaultdict(lambda: []))\n",
    "\n",
    "index = 0\n",
    "# cambridge\n",
    "for word in cambridgeDict.keys():\n",
    "    for pos in cambridgeDict[word].keys():\n",
    "        for pair in cambridgeDict[word][pos]:\n",
    "            if len(pair) == 3:\n",
    "                en, ch, phrase = pair\n",
    "                if en == '': continue\n",
    "                GPs = extractCambGP(en)\n",
    "                GPs = [ GP for GP in GPs if not (GP[2] == 'V adv' and GP[3] in unWantADV) ]\n",
    "                mapping[word][pos].append((index, list(GPs)))\n",
    "\n",
    "                file.write(str(index)+' ||| '+en+'\\t'+ch+'\\t'+phrase+'\\n')\n",
    "                index += 1 \n",
    "            else:\n",
    "                en, ch = pair\n",
    "                if en == '': continue\n",
    "                GPs = extractCambGP(en)\n",
    "                GPs = [ GP for GP in GPs if not (GP[2] == 'V adv' and GP[3] in unWantADV) ]\n",
    "                mapping[word][pos].append((index, list(GPs)))\n",
    "\n",
    "                file.write(str(index)+' ||| '+en+'\\t'+ch+'\\n')\n",
    "                index += 1 \n",
    "                \n",
    "file1.write(str(json.loads(json.dumps(mapping))))\n",
    "file.close()\n",
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_root)",
   "language": "python",
   "name": "my_root"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
